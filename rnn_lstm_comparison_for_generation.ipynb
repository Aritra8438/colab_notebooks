{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nandini `MDS202335`\n",
    "### Aritra `MCS202304`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-31T22:11:19.390648Z",
     "iopub.status.busy": "2024-10-31T22:11:19.390267Z",
     "iopub.status.idle": "2024-10-31T22:11:19.420765Z",
     "shell.execute_reply": "2024-10-31T22:11:19.419871Z",
     "shell.execute_reply.started": "2024-10-31T22:11:19.390611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:49:45.846126Z",
     "iopub.status.busy": "2024-10-31T21:49:45.845368Z",
     "iopub.status.idle": "2024-10-31T21:49:45.850944Z",
     "shell.execute_reply": "2024-10-31T21:49:45.849972Z",
     "shell.execute_reply.started": "2024-10-31T21:49:45.846084Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:49:46.760844Z",
     "iopub.status.busy": "2024-10-31T21:49:46.760318Z",
     "iopub.status.idle": "2024-10-31T21:49:46.765361Z",
     "shell.execute_reply": "2024-10-31T21:49:46.764222Z",
     "shell.execute_reply.started": "2024-10-31T21:49:46.760801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/kaggle/input/sms-spam-collection-dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:49:48.540811Z",
     "iopub.status.busy": "2024-10-31T21:49:48.540409Z",
     "iopub.status.idle": "2024-10-31T21:49:48.567725Z",
     "shell.execute_reply": "2024-10-31T21:49:48.566801Z",
     "shell.execute_reply.started": "2024-10-31T21:49:48.540767Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category                                            message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5     spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6      ham  Even my brother is not like to speak with me. ...\n",
       "7      ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8     spam  WINNER!! As a valued network customer you have...\n",
       "9     spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"spam.csv\"), encoding='latin1')\n",
    "\n",
    "df = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\n",
    "df = df.rename(columns={\"v1\": \"category\", \"v2\": \"message\"})\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An `END_TOKEN` for `LSTM` and `RNN` model to denote the `end`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "`\"❙\"` *Vertical Bold bar* is used to denote the end as it was not in the vocab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:49:53.319156Z",
     "iopub.status.busy": "2024-10-31T21:49:53.318773Z",
     "iopub.status.idle": "2024-10-31T21:49:53.324616Z",
     "shell.execute_reply": "2024-10-31T21:49:53.323520Z",
     "shell.execute_reply.started": "2024-10-31T21:49:53.319119Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❙\n"
     ]
    }
   ],
   "source": [
    "END_TOKEN = '\\u2759'\n",
    "\n",
    "print(END_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending `END_TOKEN` to every message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:49:55.427585Z",
     "iopub.status.busy": "2024-10-31T21:49:55.427128Z",
     "iopub.status.idle": "2024-10-31T21:49:55.435906Z",
     "shell.execute_reply": "2024-10-31T21:49:55.434910Z",
     "shell.execute_reply.started": "2024-10-31T21:49:55.427544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "messages = list(df[\"message\"])\n",
    "\n",
    "for idx in range(len(messages)):\n",
    "    messages[idx] += END_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:49:56.180563Z",
     "iopub.status.busy": "2024-10-31T21:49:56.179677Z",
     "iopub.status.idle": "2024-10-31T21:49:56.186158Z",
     "shell.execute_reply": "2024-10-31T21:49:56.185266Z",
     "shell.execute_reply.started": "2024-10-31T21:49:56.180521Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Sorry, I'll call later❙\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove meaningless characters to get rid of `hallucinations`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:50:10.510511Z",
     "iopub.status.busy": "2024-10-31T21:50:10.509658Z",
     "iopub.status.idle": "2024-10-31T21:50:10.516772Z",
     "shell.execute_reply": "2024-10-31T21:50:10.515805Z",
     "shell.execute_reply.started": "2024-10-31T21:50:10.510474Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "allowed_chars = [' ', '!', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '❙']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the corpus from the messages\n",
    "- All the messages are joined to get the corpus\n",
    "- Unallowed charcters are removed.\n",
    "- Corpus is truncated for faster training and relevant contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:50:40.087848Z",
     "iopub.status.busy": "2024-10-31T21:50:40.087191Z",
     "iopub.status.idle": "2024-10-31T21:50:40.704836Z",
     "shell.execute_reply": "2024-10-31T21:50:40.703798Z",
     "shell.execute_reply.started": "2024-10-31T21:50:40.087800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus has 80000 characters, 74 unique.\n"
     ]
    }
   ],
   "source": [
    "corpus = ' '.join(messages)\n",
    "corpus = ''.join(char for char in corpus if char in allowed_chars)\n",
    "corpus = corpus[:80000]\n",
    "chars = sorted(list(set(corpus))) \n",
    "\n",
    "data_size, vocab_size = len(corpus), len(chars) \n",
    "\n",
    "print(\"Corpus has {} characters, {} unique.\".format(data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:50:54.982070Z",
     "iopub.status.busy": "2024-10-31T21:50:54.981188Z",
     "iopub.status.idle": "2024-10-31T21:50:54.986629Z",
     "shell.execute_reply": "2024-10-31T21:50:54.985685Z",
     "shell.execute_reply.started": "2024-10-31T21:50:54.982026Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '❙']\n"
     ]
    }
   ],
   "source": [
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the `mapping` and the `inverse mapping` for vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:50:59.506317Z",
     "iopub.status.busy": "2024-10-31T21:50:59.505472Z",
     "iopub.status.idle": "2024-10-31T21:50:59.510739Z",
     "shell.execute_reply": "2024-10-31T21:50:59.509796Z",
     "shell.execute_reply.started": "2024-10-31T21:50:59.506278Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the stream of `characters` to sequence of `ints`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:51:02.846174Z",
     "iopub.status.busy": "2024-10-31T21:51:02.845801Z",
     "iopub.status.idle": "2024-10-31T21:51:02.851666Z",
     "shell.execute_reply": "2024-10-31T21:51:02.850505Z",
     "shell.execute_reply.started": "2024-10-31T21:51:02.846141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_sequence(input_string):\n",
    "    \"\"\"\n",
    "    Encodes a given string into a list of indices based on the char_to_ix mapping.\n",
    "\n",
    "    Parameters:\n",
    "    - input_string (str): The string to be encoded.\n",
    "    - char_to_ix (dict): A dictionary mapping characters to their respective indices.\n",
    "\n",
    "    Returns:\n",
    "    - List[int]: A list of indices corresponding to the characters in the input string.\n",
    "    \"\"\"\n",
    "    sequence = [char_to_ix[ch] for ch in input_string if ch in char_to_ix]\n",
    "    \n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:51:07.915479Z",
     "iopub.status.busy": "2024-10-31T21:51:07.915081Z",
     "iopub.status.idle": "2024-10-31T21:51:07.922956Z",
     "shell.execute_reply": "2024-10-31T21:51:07.922067Z",
     "shell.execute_reply.started": "2024-10-31T21:51:07.915442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "corpus = get_sequence(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:51:10.931887Z",
     "iopub.status.busy": "2024-10-31T21:51:10.931204Z",
     "iopub.status.idle": "2024-10-31T21:51:10.947384Z",
     "shell.execute_reply": "2024-10-31T21:51:10.946438Z",
     "shell.execute_reply.started": "2024-10-31T21:51:10.931844Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Make the corpus array tensor\n",
    "data = torch.tensor(corpus).to(device)\n",
    "\n",
    "# Add an extra dimension to dimension 1\n",
    "data = torch.unsqueeze(data, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `myRNN` Architecture\n",
    "\n",
    "The `myRNN` class implements a simple recurrent neural network with configurable layers, hidden size, and dropout:\n",
    "\n",
    "- **Parameters**:\n",
    "  - `input_size`: Number of input features in each time step of the sequence.\n",
    "  - `output_size`: Number of output features in each time step.\n",
    "  - `hidden_size`: Size of the hidden state vector.\n",
    "  - `num_layers`: Number of recurrent layers.\n",
    "  - `do_dropout`: Boolean indicating whether to apply dropout.\n",
    "\n",
    "- **Components**:\n",
    "  - **Dropout Layer**: If `do_dropout` is set to `True`, a dropout layer with a dropout rate of 0.5 is applied to the inputs, which helps prevent overfitting.\n",
    "  - **RNN Layer**: The core of the architecture, an `nn.RNN` layer, processes the input sequence using `num_layers` recurrent layers.\n",
    "  - **Decoder Layer**: An `nn.Linear` layer maps the final hidden state to the desired `output_size`, producing predictions for each time step.\n",
    "\n",
    "\n",
    "- **Forward Pass**:\n",
    "  - The input sequence is first converted to a one-hot encoding with shape `[sequence_length, batch_size, input_size]`.\n",
    "  - Dropout is applied to the input if enabled.\n",
    "  - The RNN processes the input, updating the hidden state at each step. The output of the RNN is passed to the decoder to generate predictions.\n",
    "  - The hidden state is detached to prevent gradient backpropagation through time steps across training batches.\n",
    "\n",
    "---\n",
    "\n",
    "## `myLSTM` Architecture\n",
    "\n",
    "The `myLSTM` class implements a long short-term memory network, which is more complex than the standard RNN due to additional gating mechanisms:\n",
    "\n",
    "- **Parameters**:\n",
    "  - Similar to `myRNN`: `input_size`, `output_size`, `hidden_size`, `num_layers`, and `do_dropout`.\n",
    "\n",
    "\n",
    "- **Components**:\n",
    "  - **Dropout Layer**: Functions identical to the dropout in `myRNN`.\n",
    "  - **LSTM Layer**: The `nn.LSTM` layer includes gates that control information flow, helping mitigate issues like vanishing gradients in long sequences.\n",
    "  - **Decoder Layer**: As with `myRNN`, an `nn.Linear` layer maps the hidden states to the output size for prediction.\n",
    "\n",
    "\n",
    "- **Forward Pass**:\n",
    "  - The input sequence is converted to a one-hot encoding.\n",
    "  - Dropout is applied if enabled.\n",
    "  - The LSTM processes the input, updating both the hidden state and the cell state at each step. These states are passed to the decoder for predictions.\n",
    "  - Both the hidden state and cell state are detached to avoid backpropagation through the previous batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:49:00.046304Z",
     "iopub.status.busy": "2024-10-31T21:49:00.045477Z",
     "iopub.status.idle": "2024-10-31T21:49:00.063336Z",
     "shell.execute_reply": "2024-10-31T21:49:00.062527Z",
     "shell.execute_reply.started": "2024-10-31T21:49:00.046266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class myRNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=512, num_layers=3, do_dropout=False):\n",
    "        super(myRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.do_dropout = do_dropout\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.hidden_state = None \n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        x = nn.functional.one_hot(input_seq, self.input_size).float()\n",
    "        if self.do_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x, new_hidden_state = self.rnn(x, self.hidden_state)\n",
    "        output = self.decoder(x)\n",
    "        self.hidden_state = new_hidden_state.detach() \n",
    "        return output\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        try:\n",
    "            self.load_state_dict(torch.load(path))\n",
    "        except Exception as err:\n",
    "            print(\"Error loading model from file\", path)\n",
    "            print(err)\n",
    "            print(\"Initializing model weights to default\")\n",
    "            self.__init__(self.input_size, self.output_size, self.hidden_size, self.num_layers)\n",
    "\n",
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size=512, num_layers=3, do_dropout=False):\n",
    "        super(myLSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.do_dropout = do_dropout\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.internal_state = None \n",
    "    def forward(self, input_seq):\n",
    "        x = nn.functional.one_hot(input_seq, self.input_size).float()\n",
    "        if self.do_dropout:\n",
    "            x = self.dropout(x)\n",
    "        x, new_internal_state = self.lstm(x, self.internal_state)\n",
    "        output = self.decoder(x)\n",
    "        self.internal_state = (new_internal_state[0].detach(), new_internal_state[1].detach())\n",
    "        return output\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        try:\n",
    "            self.load_state_dict(torch.load(path))\n",
    "        except Exception as err:\n",
    "            print(\"Error loading model from file\", path)\n",
    "            print(err)\n",
    "            print(\"Initializing model weights to default\")\n",
    "            self.__init__(self.input_size, self.output_size, self.hidden_size, self.num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Number of Parameters in a Model\n",
    "\n",
    "- This function calculates the total number of parameters in a given PyTorch model.\n",
    "\n",
    "- It is useful for comparing model complexity across different architectures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:51:19.271271Z",
     "iopub.status.busy": "2024-10-31T21:51:19.270883Z",
     "iopub.status.idle": "2024-10-31T21:51:19.276203Z",
     "shell.execute_reply": "2024-10-31T21:51:19.275285Z",
     "shell.execute_reply.started": "2024-10-31T21:51:19.271219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    \"\"\"\n",
    "    returns the number of parameters of a model\n",
    "    \"\"\"\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `train` function to train the LSTM and RNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:51:19.806421Z",
     "iopub.status.busy": "2024-10-31T21:51:19.806047Z",
     "iopub.status.idle": "2024-10-31T21:51:19.819532Z",
     "shell.execute_reply": "2024-10-31T21:51:19.818477Z",
     "shell.execute_reply.started": "2024-10-31T21:51:19.806385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, epoch, seq_len = 200):\n",
    "    \"\"\"\n",
    "    Train the model on a sequence data for one epoch.\n",
    "\n",
    "    This function trains the given RNN model for a single epoch using sequence data, \n",
    "    computes the average loss, and periodically displays a sample sequence generated by the model.\n",
    "\n",
    "    Parameters:\n",
    "    model : nn.Module\n",
    "        The RNN model to be trained.\n",
    "    epoch : int\n",
    "        The current epoch number, used to control logging and sample generation.\n",
    "    seq_len : int, optional\n",
    "        The length of the input sequence (default is 200).\n",
    "    \n",
    "    Returns:\n",
    "    float\n",
    "        The average training loss over all batches for the epoch.\n",
    "    \n",
    "    Process:\n",
    "    - Sets the model to training mode.\n",
    "    - Defines a cross-entropy loss function.\n",
    "    - Randomly selects a data pointer within the sequence range.\n",
    "    - For each batch in the sequence:\n",
    "      - Extracts the input and target sequences.\n",
    "      - Computes the model output and the loss.\n",
    "      - Backpropagates the loss and updates model parameters.\n",
    "    - Every 10 epochs or during early epochs (1-3), generates and displays a sample output.\n",
    "      - This sample is generated by feeding the model an initial input and sampling subsequent characters.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    test_output_len = seq_len\n",
    "    \n",
    "    data_ptr = np.random.randint(seq_len)\n",
    "    running_loss = 0\n",
    "    n = 0;\n",
    "    \n",
    "    if epoch % 10 == 0 or epoch == 1 or epoch == 2 or epoch == 3:\n",
    "        print(\"\\nStart of Epoch: {0}\".format(epoch))\n",
    "        \n",
    "    while True:\n",
    "        input_seq = data[data_ptr : data_ptr+seq_len]\n",
    "        target_seq = data[data_ptr+1 : data_ptr+seq_len+1]\n",
    "        input_seq.to(device)\n",
    "        target_seq.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_seq)\n",
    "        loss = loss_fn(torch.squeeze(output), torch.squeeze(target_seq))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        data_ptr += seq_len\n",
    "        if data_ptr + seq_len + 1 > data_size:\n",
    "            break\n",
    "        \n",
    "        n = n+1\n",
    "            \n",
    "    if epoch % 10 == 0 or epoch == 1 or epoch == 2 or epoch == 3:\n",
    "        model.eval()\n",
    "        data_ptr = 0\n",
    "\n",
    "        rand_index = np.random.randint(data_size-1)\n",
    "        input_seq = data[rand_index : rand_index+1]\n",
    "\n",
    "        \n",
    "        test_output = \"\"\n",
    "        while True:\n",
    "            output = model(input_seq)\n",
    "\n",
    "            output = F.softmax(torch.squeeze(output), dim=0)\n",
    "            dist = Categorical(output)\n",
    "            index = dist.sample().item()\n",
    "            \n",
    "\n",
    "            test_output += ix_to_char[index]\n",
    "\n",
    "            input_seq[0][0] = index\n",
    "            data_ptr += 1\n",
    "\n",
    "            if data_ptr > test_output_len:\n",
    "                break\n",
    "        print(\"----------\")\n",
    "        print(\"TRAIN Sample\")\n",
    "        print(test_output)\n",
    "        print(\"----------\")\n",
    "        print(\"End of Epoch: {0} \\t Loss: {1:.8f}\".format(epoch, running_loss / n))\n",
    "    \n",
    "    return running_loss / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `test` function to evaluate the performance of message completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:01:51.006898Z",
     "iopub.status.busy": "2024-10-31T22:01:51.006478Z",
     "iopub.status.idle": "2024-10-31T22:01:51.018404Z",
     "shell.execute_reply": "2024-10-31T22:01:51.017496Z",
     "shell.execute_reply.started": "2024-10-31T22:01:51.006858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test(model, x, output_len=50, T=0): \n",
    "    \"\"\"\n",
    "    Generate a sequence of characters using the provided model based on an initial input string.\n",
    "\n",
    "    This function takes an initial input string, encodes it as indices, and uses the \n",
    "    specified RNN model to generate a sequence of characters of a given length. \n",
    "    The generation can be influenced by the temperature parameter (T) to control randomness.\n",
    "\n",
    "    Parameters:\n",
    "    model : nn.Module\n",
    "        The trained RNN model to be used for character generation.\n",
    "    x : str\n",
    "        The initial input string from which to start generating the output sequence.\n",
    "    output_len : int, optional\n",
    "        The maximum length of the generated output sequence (default is 50).\n",
    "    T : float, optional\n",
    "        The temperature parameter that influences the randomness of the predictions. \n",
    "        A value of 0 means that the model will always select the most probable character \n",
    "        (greedy approach). A value other than 1 will sample from the output probability distribution.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    Process:\n",
    "    - The model is set to evaluation mode.\n",
    "    - The input string is converted to a tensor of character indices.\n",
    "    - A subset of the input sequence is selected to be used for generation.\n",
    "    - The model generates characters one at a time until the specified output length \n",
    "      is reached or an end token is encountered.\n",
    "    - The generated characters are accumulated into a result string and printed along \n",
    "      with the initial input context.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    data_ptr = 0\n",
    "    hidden_state = None    \n",
    "    \n",
    "    \n",
    "    input_seq = torch.tensor([char_to_ix[char] for char in x]).to(device)\n",
    "    input_seq = torch.unsqueeze(input_seq, dim=1)\n",
    "    y = 30\n",
    "    y = min(y, len(input_seq) // 2)\n",
    "    x = 0\n",
    "    input_seq = input_seq[x:x+y]\n",
    "    test_input = ''.join(ix_to_char[int(ix)] for ix in input_seq)\n",
    "\n",
    "    output = model(input_seq)\n",
    "    \n",
    "    input_seq = data[x+y:x+y+1]\n",
    "    \n",
    "    test_output = \"\"\n",
    "    \n",
    "    while True:\n",
    "        output = model(input_seq)\n",
    "        \n",
    "        output = F.softmax(torch.squeeze(output), dim=0)\n",
    "        dist = Categorical(output)\n",
    "        index = dist.sample().item()\n",
    "        \n",
    "        most_probable_index = torch.argmax(output).item()\n",
    "        \n",
    "        # If temperature T is set to 0, we take the most probable one always.\n",
    "        if T == 0:\n",
    "            index = most_probable_index\n",
    "        test_output += ix_to_char[index]\n",
    "        \n",
    "        if ix_to_char[index] == END_TOKEN:\n",
    "            break\n",
    "        \n",
    "        input_seq[0][0] = index\n",
    "        data_ptr += 1\n",
    "        \n",
    "        if data_ptr  > output_len:\n",
    "            break\n",
    "\n",
    "    print(f\"{model.__class__.__name__} completed:\")\n",
    "    print(f\"context: {test_input}\")\n",
    "    print(f\"output: {test_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T09:06:08.781035Z",
     "iopub.status.busy": "2024-10-31T09:06:08.780666Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e9b78b12774915a726d978680b16b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of Epoch: 1\n",
      "----------\n",
      "TRAIN Sample\n",
      "lock that intr u tear of doing tho end to MANESS! I\n",
      "----------\n",
      "End of Epoch: 1 \t Loss: 0.28820632\n",
      "\n",
      "Start of Epoch: 2\n",
      "----------\n",
      "TRAIN Sample\n",
      "ok, I beilge in the pls on my somect.❙ I lave m che\n",
      "----------\n",
      "End of Epoch: 2 \t Loss: 0.22217210\n",
      "\n",
      "Start of Epoch: 3\n",
      "----------\n",
      "TRAIN Sample\n",
      "ng about the mistumf ot the u'll then i go soworrow\n",
      "----------\n",
      "End of Epoch: 3 \t Loss: 0.13288940\n",
      "\n",
      "Start of Epoch: 10\n",
      "----------\n",
      "TRAIN Sample\n",
      "re.❙ As per u bit of people that dont kear and wank\n",
      "----------\n",
      "End of Epoch: 10 \t Loss: 0.01264650\n",
      "\n",
      "Start of Epoch: 20\n",
      "----------\n",
      "TRAIN Sample\n",
      "er already?❙ Yup... Ok lar... Joking wif u oni...❙ \n",
      "----------\n",
      "End of Epoch: 20 \t Loss: 0.02349879\n",
      "\n",
      "Start of Epoch: 30\n",
      "----------\n",
      "TRAIN Sample\n",
      "that be a win FA Cup final tkts 2 a wAly comp to wi\n",
      "----------\n",
      "End of Epoch: 30 \t Loss: 0.05197851\n",
      "\n",
      "Start of Epoch: 40\n",
      "----------\n",
      "TRAIN Sample\n",
      "rconter! Nit me know so nice is so itker ereding❙ Y\n",
      "----------\n",
      "End of Epoch: 40 \t Loss: 0.14801698\n",
      "\n",
      "Start of Epoch: 50\n",
      "----------\n",
      "TRAIN Sample\n",
      "e coans if thats the way its gota b❙ Even my sign, \n",
      "----------\n",
      "End of Epoch: 50 \t Loss: 0.06381190\n",
      "\n",
      "Start of Epoch: 60\n",
      "----------\n",
      "TRAIN Sample\n",
      "e or that that's leare that dixls❙ You like dire th\n",
      "----------\n",
      "End of Epoch: 60 \t Loss: 0.02203953\n",
      "\n",
      "Start of Epoch: 70\n",
      "----------\n",
      "TRAIN Sample\n",
      "howey account has been refilled su nire with m... ❙\n",
      "----------\n",
      "End of Epoch: 70 \t Loss: 0.02347969\n",
      "\n",
      "Start of Epoch: 80\n",
      "----------\n",
      "TRAIN Sample\n",
      ".. As I entered my house now...❙ FreeMsg Why haven'\n",
      "----------\n",
      "End of Epoch: 80 \t Loss: 0.03963449\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 512\n",
    "num_layers = 3       \n",
    "lr = 0.002          \n",
    "\n",
    "model_lstm = myLSTM(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
    "optimizer = torch.optim.Adam(model_lstm.parameters(), lr=lr)\n",
    "\n",
    "best_model_lstm = myLSTM(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
    "best_lstm_loss = 10000\n",
    " \n",
    "for epoch in tqdm(range(1, 101)):\n",
    "    epoch_loss = train(model_lstm, epoch, 50)\n",
    "    if epoch_loss < best_lstm_loss:\n",
    "        best_lstm_loss = epoch_loss\n",
    "        best_model_lstm.load_state_dict(model_lstm.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T08:56:50.513539Z",
     "iopub.status.busy": "2024-10-31T08:56:50.513149Z",
     "iopub.status.idle": "2024-10-31T08:56:50.520790Z",
     "shell.execute_reply": "2024-10-31T08:56:50.519906Z",
     "shell.execute_reply.started": "2024-10-31T08:56:50.513503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def complete_message_using_lstm(index, T=0):\n",
    "    \"\"\"\n",
    "    A function that takes the first half of the message to return the remaining part.\n",
    "    \"\"\"\n",
    "    print(\"original message:\")\n",
    "    print(messages[index])\n",
    "    test(best_model_lstm, x=messages[index], T=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of the `LSTM` on the unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T08:58:11.312445Z",
     "iopub.status.busy": "2024-10-31T08:58:11.311872Z",
     "iopub.status.idle": "2024-10-31T08:58:11.371215Z",
     "shell.execute_reply": "2024-10-31T08:58:11.370315Z",
     "shell.execute_reply.started": "2024-10-31T08:58:11.312406Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "Probably money worries. Things are coming due and i have several outstanding invoices for work i did two and three months ago.❙\n",
      "LSTM completed:\n",
      "context: Probably money worries. Things\n",
      "output: for making me ? I been egilly selected 2 receive 10\n"
     ]
    }
   ],
   "source": [
    "complete_message_using_lstm(4675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T08:43:47.199929Z",
     "iopub.status.busy": "2024-10-31T08:43:47.199569Z",
     "iopub.status.idle": "2024-10-31T08:43:47.259097Z",
     "shell.execute_reply": "2024-10-31T08:43:47.258238Z",
     "shell.execute_reply.started": "2024-10-31T08:43:47.199895Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "Aight, I'll hit you up when I get some cash❙\n",
      "LSTM completed:\n",
      "context: Aight, I'll hit you up when I \n",
      "output: ervice representative on 0800 169 6031 between 10am\n"
     ]
    }
   ],
   "source": [
    "complete_message_using_lstm(3786)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T08:43:59.202791Z",
     "iopub.status.busy": "2024-10-31T08:43:59.201667Z",
     "iopub.status.idle": "2024-10-31T08:43:59.262724Z",
     "shell.execute_reply": "2024-10-31T08:43:59.261839Z",
     "shell.execute_reply.started": "2024-10-31T08:43:59.202745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "I am waiting machan. Call me once you free.❙\n",
      "LSTM completed:\n",
      "context: I am waiting machan. Call me o\n",
      "output: ce ic Smile Qursh on 5800 169 6031 between 10am-9pm\n"
     ]
    }
   ],
   "source": [
    "complete_message_using_lstm(5663)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T08:50:22.746223Z",
     "iopub.status.busy": "2024-10-31T08:50:22.745787Z",
     "iopub.status.idle": "2024-10-31T08:50:22.809109Z",
     "shell.execute_reply": "2024-10-31T08:50:22.808214Z",
     "shell.execute_reply.started": "2024-10-31T08:50:22.746157Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "Hope you are having a great new semester. Do wish you the very best. You are made for greatness.❙\n",
      "LSTM completed:\n",
      "context: Hope you are having a great ne\n",
      "output:  You are a winner U have been specially selected 2 \n"
     ]
    }
   ],
   "source": [
    "complete_message_using_lstm(1239)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of the `LSTM` on the previously seen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T08:46:18.295613Z",
     "iopub.status.busy": "2024-10-31T08:46:18.295231Z",
     "iopub.status.idle": "2024-10-31T08:46:18.355251Z",
     "shell.execute_reply": "2024-10-31T08:46:18.354360Z",
     "shell.execute_reply.started": "2024-10-31T08:46:18.295577Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...❙\n",
      "LSTM completed:\n",
      "context: Go until jurong point, crazy..\n",
      "output: Alright you liked it, since i was doing the best i \n"
     ]
    }
   ],
   "source": [
    "complete_message_using_lstm(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T08:46:31.502730Z",
     "iopub.status.busy": "2024-10-31T08:46:31.501991Z",
     "iopub.status.idle": "2024-10-31T08:46:31.555788Z",
     "shell.execute_reply": "2024-10-31T08:46:31.554921Z",
     "shell.execute_reply.started": "2024-10-31T08:46:31.502690Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.❙\n",
      "LSTM completed:\n",
      "context: I'm gonna be home soon and i d\n",
      "output: es not the money so carlos can make the call❙\n"
     ]
    }
   ],
   "source": [
    "complete_message_using_lstm(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T08:49:40.845300Z",
     "iopub.status.busy": "2024-10-31T08:49:40.844914Z",
     "iopub.status.idle": "2024-10-31T08:49:40.850360Z",
     "shell.execute_reply": "2024-10-31T08:49:40.849509Z",
     "shell.execute_reply.started": "2024-10-31T08:49:40.845264Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "Lol ok. I'll snatch her purse too.❙\n",
      "LSTM completed:\n",
      "context: Lol ok. I'll snatch her purse \n",
      "output: How?❙\n"
     ]
    }
   ],
   "source": [
    "complete_message_using_lstm(1233)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T21:52:18.830259Z",
     "iopub.status.busy": "2024-10-31T21:52:18.829869Z",
     "iopub.status.idle": "2024-10-31T22:00:16.073039Z",
     "shell.execute_reply": "2024-10-31T22:00:16.072102Z",
     "shell.execute_reply.started": "2024-10-31T21:52:18.830217Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86504e6b8a4f4b4c929131c8ab78f581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of Epoch: 1\n",
      "----------\n",
      "TRAIN Sample\n",
      "G np c agor.rLgaa osTei  esg E  dan a-d.i possoos i\n",
      "----------\n",
      "End of Epoch: 1 \t Loss: 3.84326125\n",
      "\n",
      "Start of Epoch: 2\n",
      "----------\n",
      "TRAIN Sample\n",
      "imuiaEuaz ppigaozcntbir .tW ialf tgupwno? uu ipL au\n",
      "----------\n",
      "End of Epoch: 2 \t Loss: 3.81743740\n",
      "\n",
      "Start of Epoch: 3\n",
      "----------\n",
      "TRAIN Sample\n",
      "leowdddtoogi odaoiooea  oetddnociimg terrna gsrdit \n",
      "----------\n",
      "End of Epoch: 3 \t Loss: 3.81418707\n",
      "\n",
      "Start of Epoch: 10\n",
      "----------\n",
      "TRAIN Sample\n",
      "0 c  aicntpo rds rn  p gaRswiaioutt gr tdoceedn iud\n",
      "----------\n",
      "End of Epoch: 10 \t Loss: 3.81446393\n",
      "\n",
      "Start of Epoch: 20\n",
      "----------\n",
      "TRAIN Sample\n",
      " s0pr  ooer0rns osigwiidr  onRi ggidrcntaBdla ts nL\n",
      "----------\n",
      "End of Epoch: 20 \t Loss: 3.81933014\n",
      "\n",
      "Start of Epoch: 30\n",
      "----------\n",
      "TRAIN Sample\n",
      "idzoRmurziisncgatus ddcraL emtdRi uncgcnir eae  zii\n",
      "----------\n",
      "End of Epoch: 30 \t Loss: 3.81765289\n",
      "\n",
      "Start of Epoch: 40\n",
      "----------\n",
      "TRAIN Sample\n",
      "o .n e gugeptp din nidni B r urruam  roir0ntrcdc .m\n",
      "----------\n",
      "End of Epoch: 40 \t Loss: 3.81746325\n",
      "\n",
      "Start of Epoch: 50\n",
      "----------\n",
      "TRAIN Sample\n",
      "ntL trHnoinls.ircp icdrnsi.is' pniuiiprtd   itr shn\n",
      "----------\n",
      "End of Epoch: 50 \t Loss: 3.81766674\n",
      "\n",
      "Start of Epoch: 60\n",
      "----------\n",
      "TRAIN Sample\n",
      "oRogp tsBdorn   d i ig a FctBiirns0r natzimicardg s\n",
      "----------\n",
      "End of Epoch: 60 \t Loss: 3.81897489\n",
      "\n",
      "Start of Epoch: 70\n",
      "----------\n",
      "TRAIN Sample\n",
      "adam ngriae iccss mont 0. issgnnm  ttsar iie nLnnua\n",
      "----------\n",
      "End of Epoch: 70 \t Loss: 3.81341853\n",
      "\n",
      "Start of Epoch: 80\n",
      "----------\n",
      "TRAIN Sample\n",
      "ridn igo RadddBBtacoiiggrnocdgrpo❙n  y.atzrt ❙iB B4\n",
      "----------\n",
      "End of Epoch: 80 \t Loss: 3.81509906\n",
      "\n",
      "Start of Epoch: 90\n",
      "----------\n",
      "TRAIN Sample\n",
      " dinRidapiGLw.n  nmi❙trnRudrospgrd.aLssaaretrora sn\n",
      "----------\n",
      "End of Epoch: 90 \t Loss: 3.81369062\n",
      "\n",
      "Start of Epoch: 100\n",
      "----------\n",
      "TRAIN Sample\n",
      "   a noi nrrsiiu diauiddm n!osun3nitrpshgJcaJco L!w\n",
      "----------\n",
      "End of Epoch: 100 \t Loss: 3.81933128\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 1024 \n",
    "num_layers = 3       \n",
    "lr = 0.002          \n",
    "\n",
    "model_rnn = myRNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=lr)\n",
    "\n",
    "best_model_rnn = myRNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\n",
    "best_rnn_loss = 10000\n",
    " \n",
    "for epoch in tqdm(range(1, 101)):\n",
    "    epoch_loss = train(model_rnn, epoch, 50)\n",
    "    if epoch_loss < best_rnn_loss:\n",
    "        best_rnn_loss = epoch_loss\n",
    "        best_model_rnn.load_state_dict(model_rnn.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:02:03.822916Z",
     "iopub.status.busy": "2024-10-31T22:02:03.822289Z",
     "iopub.status.idle": "2024-10-31T22:02:03.827646Z",
     "shell.execute_reply": "2024-10-31T22:02:03.826688Z",
     "shell.execute_reply.started": "2024-10-31T22:02:03.822872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def complete_message_using_rnn(index):\n",
    "    \"\"\"\n",
    "    A function that takes the first half of the message to return the remaining part.\n",
    "    \"\"\"\n",
    "    print(\"original message:\")\n",
    "    print(messages[index])\n",
    "    test(best_model_rnn, x=messages[index], T=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of the `RNN` on the unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:03:39.618972Z",
     "iopub.status.busy": "2024-10-31T22:03:39.618252Z",
     "iopub.status.idle": "2024-10-31T22:03:39.641988Z",
     "shell.execute_reply": "2024-10-31T22:03:39.640964Z",
     "shell.execute_reply.started": "2024-10-31T22:03:39.618928Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "Juz go google n search 4 qet...❙\n",
      "myRNN completed:\n",
      "context: Juz go google n \n",
      "output:  t.iadnsaL❙\n"
     ]
    }
   ],
   "source": [
    "complete_message_using_rnn(4399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:03:43.409461Z",
     "iopub.status.busy": "2024-10-31T22:03:43.408613Z",
     "iopub.status.idle": "2024-10-31T22:03:43.450861Z",
     "shell.execute_reply": "2024-10-31T22:03:43.449780Z",
     "shell.execute_reply.started": "2024-10-31T22:03:43.409421Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "Do you still have the grinder?❙\n",
      "myRNN completed:\n",
      "context: Do you still ha\n",
      "output: scrr rrpi dnoecnncobiiot paooiss❙\n"
     ]
    }
   ],
   "source": [
    "complete_message_using_rnn(2987)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:03:46.594624Z",
     "iopub.status.busy": "2024-10-31T22:03:46.594269Z",
     "iopub.status.idle": "2024-10-31T22:03:46.655986Z",
     "shell.execute_reply": "2024-10-31T22:03:46.655085Z",
     "shell.execute_reply.started": "2024-10-31T22:03:46.594590Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "Try to do something dear. You read something for exams❙\n",
      "myRNN completed:\n",
      "context: Try to do something dear. Y\n",
      "output: ?eBiitiodrdsn int senongyaiiw aagoinnLmrfEunogngcd \n"
     ]
    }
   ],
   "source": [
    "complete_message_using_rnn(4553)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:04:05.143341Z",
     "iopub.status.busy": "2024-10-31T22:04:05.142245Z",
     "iopub.status.idle": "2024-10-31T22:04:05.209524Z",
     "shell.execute_reply": "2024-10-31T22:04:05.208622Z",
     "shell.execute_reply.started": "2024-10-31T22:04:05.143289Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "I thank you so much for all you do with selflessness. I love you plenty.❙\n",
      "myRNN completed:\n",
      "context: I thank you so much for all yo\n",
      "output: nidni niantodp.gnhtno  od .np ssdi np ndcaw tdp.sza\n"
     ]
    }
   ],
   "source": [
    "complete_message_using_rnn(3019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of the `RNN` on the previously seen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:02:15.306420Z",
     "iopub.status.busy": "2024-10-31T22:02:15.305786Z",
     "iopub.status.idle": "2024-10-31T22:02:15.368828Z",
     "shell.execute_reply": "2024-10-31T22:02:15.367930Z",
     "shell.execute_reply.started": "2024-10-31T22:02:15.306380Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...❙\n",
      "myRNN completed:\n",
      "context: Go until jurong point, crazy..\n",
      "output: iIigs  .ae  snnpHnnomsonkt GGcgdttnL sniinsghddoi g\n"
     ]
    }
   ],
   "source": [
    "complete_message_using_rnn(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:04:11.626776Z",
     "iopub.status.busy": "2024-10-31T22:04:11.626123Z",
     "iopub.status.idle": "2024-10-31T22:04:11.639499Z",
     "shell.execute_reply": "2024-10-31T22:04:11.638363Z",
     "shell.execute_reply.started": "2024-10-31T22:04:11.626714Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.❙\n",
      "myRNN completed:\n",
      "context: I've been searching for the ri\n",
      "output: ❙\n"
     ]
    }
   ],
   "source": [
    "complete_message_using_rnn(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:04:21.109684Z",
     "iopub.status.busy": "2024-10-31T22:04:21.109311Z",
     "iopub.status.idle": "2024-10-31T22:04:21.169902Z",
     "shell.execute_reply": "2024-10-31T22:04:21.168963Z",
     "shell.execute_reply.started": "2024-10-31T22:04:21.109648Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "Ok lar i double check wif da hair dresser already he said wun cut v short. He said will cut until i look nice.❙\n",
      "myRNN completed:\n",
      "context: Ok lar i double check wif da h\n",
      "output: iesrdstatnndr MagLrutnnitaintntsgdn oemn uwunscinci\n"
     ]
    }
   ],
   "source": [
    "complete_message_using_rnn(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:11:24.403553Z",
     "iopub.status.busy": "2024-10-31T22:11:24.402597Z",
     "iopub.status.idle": "2024-10-31T22:11:24.409510Z",
     "shell.execute_reply": "2024-10-31T22:11:24.408492Z",
     "shell.execute_reply.started": "2024-10-31T22:11:24.403510Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------+\n",
      "|   Model    | Number of Parameters |\n",
      "+------------+----------------------+\n",
      "| RNN Model  |       5400650        |\n",
      "| LSTM Model |       5444682        |\n",
      "+------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "rnn_params = get_n_params(best_model_rnn)\n",
    "lstm_params = get_n_params(best_model_lstm)\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Model\", \"Number of Parameters\"]\n",
    "table.add_row([\"RNN Model\", rnn_params])\n",
    "table.add_row([\"LSTM Model\", lstm_params])\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between `LSTM` and `RNN` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:18:55.422280Z",
     "iopub.status.busy": "2024-10-31T22:18:55.421438Z",
     "iopub.status.idle": "2024-10-31T22:18:55.427543Z",
     "shell.execute_reply": "2024-10-31T22:18:55.426597Z",
     "shell.execute_reply.started": "2024-10-31T22:18:55.422241Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compare_completion(index):\n",
    "    \"\"\"\n",
    "    A function that takes the first half of the message and compares rnn and lstm output for the remaining part.\n",
    "    \"\"\"\n",
    "    print(\"original message:\")\n",
    "    print(messages[index])\n",
    "    print(\"\\n\")\n",
    "    test(best_model_rnn, x=messages[index], T=1)\n",
    "    print(\"\\n\")\n",
    "    test(best_model_lstm, x=messages[index], T=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:18:56.222279Z",
     "iopub.status.busy": "2024-10-31T22:18:56.221617Z",
     "iopub.status.idle": "2024-10-31T22:18:56.323496Z",
     "shell.execute_reply": "2024-10-31T22:18:56.322569Z",
     "shell.execute_reply.started": "2024-10-31T22:18:56.222239Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "He's just gonna worry for nothing. And he won't give you money its no use.❙\n",
      "\n",
      "\n",
      "myRNN completed:\n",
      "context: He's just gonna worry for noth\n",
      "output: oLlnu inisntnnagatonr.nmgo .L  z.tai  cu .iaitnnrs.\n",
      "\n",
      "\n",
      "myLSTM completed:\n",
      "context: He's just gonna worry for noth\n",
      "output:  Txll be show you sorry your reply...❙\n"
     ]
    }
   ],
   "source": [
    "compare_completion(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:19:22.001014Z",
     "iopub.status.busy": "2024-10-31T22:19:22.000610Z",
     "iopub.status.idle": "2024-10-31T22:19:22.115072Z",
     "shell.execute_reply": "2024-10-31T22:19:22.114197Z",
     "shell.execute_reply.started": "2024-10-31T22:19:22.000974Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "hiya hows it going in sunny africa? hope u r avin a good time. give that big old silver back a big kiss from me.❙\n",
      "\n",
      "\n",
      "myRNN completed:\n",
      "context: hiya hows it going in sunny af\n",
      "output: d!caatogtty rtIwadh.  gtnBncana mnns..nenid amomtgn\n",
      "\n",
      "\n",
      "myLSTM completed:\n",
      "context: hiya hows it going in sunny af\n",
      "output:  that i have hairdressers appointment at four so ne\n"
     ]
    }
   ],
   "source": [
    "compare_completion(4567)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:20:11.576608Z",
     "iopub.status.busy": "2024-10-31T22:20:11.575995Z",
     "iopub.status.idle": "2024-10-31T22:20:11.701431Z",
     "shell.execute_reply": "2024-10-31T22:20:11.700496Z",
     "shell.execute_reply.started": "2024-10-31T22:20:11.576568Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "This is all just creepy and crazy to me.❙\n",
      "\n",
      "\n",
      "myRNN completed:\n",
      "context: This is all just cre\n",
      "output: oisn d oaudo iensrouruggn1rte.0ie iad w Rsrdtpiip i\n",
      "\n",
      "\n",
      "myLSTM completed:\n",
      "context: This is all just cre\n",
      "output: sht? He said will cut until i look nice.❙\n"
     ]
    }
   ],
   "source": [
    "compare_completion(3097)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:23:05.047968Z",
     "iopub.status.busy": "2024-10-31T22:23:05.047584Z",
     "iopub.status.idle": "2024-10-31T22:23:05.162587Z",
     "shell.execute_reply": "2024-10-31T22:23:05.161647Z",
     "shell.execute_reply.started": "2024-10-31T22:23:05.047932Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "When/where do I pick you up❙\n",
      "\n",
      "\n",
      "myRNN completed:\n",
      "context: When/where do \n",
      "output:   . aa tan zgroyH  atag RegBsd tsr.o prnepoea zemei\n",
      "\n",
      "\n",
      "myLSTM completed:\n",
      "context: When/where do \n",
      "output: t seemed so happy about the cave. I'm sorry i did. \n"
     ]
    }
   ],
   "source": [
    "compare_completion(4880)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:23:32.916676Z",
     "iopub.status.busy": "2024-10-31T22:23:32.915788Z",
     "iopub.status.idle": "2024-10-31T22:23:33.029420Z",
     "shell.execute_reply": "2024-10-31T22:23:33.028459Z",
     "shell.execute_reply.started": "2024-10-31T22:23:32.916635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "I'm reaching home in 5 min.❙\n",
      "\n",
      "\n",
      "myRNN completed:\n",
      "context: I'm reaching h\n",
      "output: o  ndLddsGnpco -ta  caLtoogppncnnna oanato dahttiio\n",
      "\n",
      "\n",
      "myLSTM completed:\n",
      "context: I'm reaching h\n",
      "output: w dying an egg ? Did you makeoa tea? Are you eating\n"
     ]
    }
   ],
   "source": [
    "compare_completion(4006)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison on seen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:21:13.944606Z",
     "iopub.status.busy": "2024-10-31T22:21:13.943864Z",
     "iopub.status.idle": "2024-10-31T22:21:14.029168Z",
     "shell.execute_reply": "2024-10-31T22:21:14.028266Z",
     "shell.execute_reply.started": "2024-10-31T22:21:13.944569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "Pls go ahead with watts. I just wanted to be sure. Do have a great weekend. Abiola❙\n",
      "\n",
      "\n",
      "myRNN completed:\n",
      "context: Pls go ahead with watts. I jus\n",
      "output: oari s nidzi anGog s❙\n",
      "\n",
      "\n",
      "myLSTM completed:\n",
      "context: Pls go ahead with watts. I jus\n",
      "output:  wanted to be sure. Do have a great weekend. Abiola\n"
     ]
    }
   ],
   "source": [
    "compare_completion(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:23:56.814168Z",
     "iopub.status.busy": "2024-10-31T22:23:56.813321Z",
     "iopub.status.idle": "2024-10-31T22:23:56.935658Z",
     "shell.execute_reply": "2024-10-31T22:23:56.934583Z",
     "shell.execute_reply.started": "2024-10-31T22:23:56.814116Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...❙\n",
      "\n",
      "\n",
      "myRNN completed:\n",
      "context: Go until jurong point, crazy..\n",
      "output: onrtRaSriarwsr.i.csdccssirp rgs.htsBduBsoa-e pao.ao\n",
      "\n",
      "\n",
      "myLSTM completed:\n",
      "context: Go until jurong point, crazy..\n",
      "output: Available only in bosty no hand did? Quick have a c\n"
     ]
    }
   ],
   "source": [
    "compare_completion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:24:15.169637Z",
     "iopub.status.busy": "2024-10-31T22:24:15.169278Z",
     "iopub.status.idle": "2024-10-31T22:24:15.285832Z",
     "shell.execute_reply": "2024-10-31T22:24:15.284907Z",
     "shell.execute_reply.started": "2024-10-31T22:24:15.169603Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.❙\n",
      "\n",
      "\n",
      "myRNN completed:\n",
      "context: I'm gonna be home soon and i d\n",
      "output: Lia hhrpamd7htiis .sg reggtrnnoMtngdirngndot   ezpe\n",
      "\n",
      "\n",
      "myLSTM completed:\n",
      "context: I'm gonna be home soon and i d\n",
      "output: er that i hours on your man well check all thk i co\n"
     ]
    }
   ],
   "source": [
    "compare_completion(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:24:30.336690Z",
     "iopub.status.busy": "2024-10-31T22:24:30.336025Z",
     "iopub.status.idle": "2024-10-31T22:24:30.447722Z",
     "shell.execute_reply": "2024-10-31T22:24:30.446867Z",
     "shell.execute_reply.started": "2024-10-31T22:24:30.336650Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "Lol ok. I'll snatch her purse too.❙\n",
      "\n",
      "\n",
      "myRNN completed:\n",
      "context: Lol ok. I'll snat\n",
      "output: hn L tzi antcioopiz.hdsiituuncp nLngniitBn ets os s\n",
      "\n",
      "\n",
      "myLSTM completed:\n",
      "context: Lol ok. I'll snat\n",
      "output:  a 1500 prize Jackpot! Txt ur national team to 8707\n"
     ]
    }
   ],
   "source": [
    "compare_completion(1233)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:24:49.151310Z",
     "iopub.status.busy": "2024-10-31T22:24:49.150931Z",
     "iopub.status.idle": "2024-10-31T22:24:49.266701Z",
     "shell.execute_reply": "2024-10-31T22:24:49.265794Z",
     "shell.execute_reply.started": "2024-10-31T22:24:49.151273Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "Hope you are having a great new semester. Do wish you the very best. You are made for greatness.❙\n",
      "\n",
      "\n",
      "myRNN completed:\n",
      "context: Hope you are having a great ne\n",
      "output: oo icuigcpiiR L.tinctigssLidn odOttrddgotp ndSismut\n",
      "\n",
      "\n",
      "myLSTM completed:\n",
      "context: Hope you are having a great ne\n",
      "output:  this saymur star sign, e. go ahead with watts. I j\n"
     ]
    }
   ],
   "source": [
    "compare_completion(1239)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-31T22:24:59.173931Z",
     "iopub.status.busy": "2024-10-31T22:24:59.173526Z",
     "iopub.status.idle": "2024-10-31T22:24:59.257091Z",
     "shell.execute_reply": "2024-10-31T22:24:59.256163Z",
     "shell.execute_reply.started": "2024-10-31T22:24:59.173893Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original message:\n",
      "What you thinked about me. First time you saw me in class.❙\n",
      "\n",
      "\n",
      "myRNN completed:\n",
      "context: What you thinked about me. Fi\n",
      "output:  sonosoaDnri et   odsegstew a totpfhoG❙\n",
      "\n",
      "\n",
      "myLSTM completed:\n",
      "context: What you thinked about me. Fi\n",
      "output: st time you saw me in class.❙\n"
     ]
    }
   ],
   "source": [
    "compare_completion(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We can see the `LSTM` works far better than `RNN` when use as a `generator`.\n",
    "\n",
    "- From the output on seen data, `LSTM` din't memorize the sequence, yet yielded completions of moderately high quality, even the sentences were meaningful for `LSTM`. Such a high-quality completion is shown below.\n",
    " ```\n",
    " original message:\n",
    "What you thinked about me. First time you saw me in class.❙\n",
    "\n",
    "---\n",
    "myRNN completed:\n",
    "context: What you thinked about me. Fi\n",
    "output:  sonosoaDnri et   odsegstew a totpfhoG❙\n",
    "---\n",
    "myLSTM completed:\n",
    "context: What you thinked about me. Fi\n",
    "output: st time you saw me in class.❙\n",
    "```\n",
    "- In contrast, `RNN` generated gibberish texts all the time.\n",
    "\n",
    "So, we can infer that `long term memory` of `LSTM` really made a difference."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 483,
     "sourceId": 982,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
